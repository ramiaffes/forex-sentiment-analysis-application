{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df59dab9-d42e-46b6-aeac-db40e258aeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea87e21-5afd-4fe6-af1c-3f98f7594432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at XLNet-large-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('XLNet-large-cased')\n",
    "model = XLNetForSequenceClassification.from_pretrained('XLNet-large-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a963389-c0aa-4c0f-afa0-365d63388cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/forex_news.csv')\n",
    "X = df['news'].tolist()\n",
    "label_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047d921d-9747-43e7-9eb5-b5d1e46006af",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df['labels'].map(label_mapping).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15515b65-bb00-4815-a9a7-78da59becf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics (accuracy & F1-score)\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13cf3cc2-d5a9-46d5-ac68-b7cb75b92287",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "texts=X\n",
    "for text in texts:\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=46, return_tensors=\"pt\")\n",
    "    input_ids.append(encoding.input_ids)\n",
    "    attention_masks.append(encoding.attention_mask)\n",
    "input_ids = torch.cat(input_ids, dim=0).to('cuda')\n",
    "attention_masks = torch.cat(attention_masks, dim=0).to('cuda')\n",
    "model.to('cuda')\n",
    "labels = torch.tensor(labels).to('cuda')\n",
    "# Split the data into train, validation, and test sets\n",
    "train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17356359-8d96-4f84-8bc7-3fd44d53309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe70434-34f7-44e4-8215-511fd0f3b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [13:57<00:00,  3.24s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:37<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Avg. Training Loss: 1.0872, Validation Accuracy: 0.4864, f1: 0.3184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [13:57<00:00,  3.25s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:37<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Avg. Training Loss: 0.9382, Validation Accuracy: 0.5039, f1: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [14:36<00:00,  3.40s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:41<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: Avg. Training Loss: 0.7916, Validation Accuracy: 0.6483, f1: 0.6329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [15:48<00:00,  3.68s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [02:54<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: Avg. Training Loss: 0.6338, Validation Accuracy: 0.7733, f1: 0.7747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [14:42<00:00,  3.42s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:36<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: Avg. Training Loss: 0.4670, Validation Accuracy: 0.7868, f1: 0.7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [14:27<00:00,  3.36s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:36<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: Avg. Training Loss: 0.3444, Validation Accuracy: 0.7955, f1: 0.7966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [14:19<00:00,  3.33s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:35<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: Avg. Training Loss: 0.2639, Validation Accuracy: 0.7723, f1: 0.7764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [17:23<00:00,  4.04s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:39<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: Avg. Training Loss: 0.1945, Validation Accuracy: 0.7868, f1: 0.7910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [19:44<00:00,  4.59s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:35<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: Avg. Training Loss: 0.1560, Validation Accuracy: 0.8091, f1: 0.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 258/258 [13:39<00:00,  3.18s/it]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████████████████| 65/65 [00:36<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: Avg. Training Loss: 0.1110, Validation Accuracy: 0.8101, f1: 0.8126\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and loss function\n",
    "weight_decay=0.01\n",
    "epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5,weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        labels = labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits \n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    nb_eval_steps = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for batch in tqdm(test_dataloader, desc=\"Validation\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        labels = labels.long()\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        predicted_labels2 = torch.argmax(logits, dim=1)\n",
    "        nb_eval_steps += 1\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted_labels2.cpu().numpy())\n",
    "    f1 = f1_metric.compute(predictions=predicted_labels, references=true_labels, average=\"weighted\")[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predicted_labels, references=true_labels)[\"accuracy\"]\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}: Avg. Training Loss: {avg_train_loss:.4f}, Validation Accuracy: {accuracy:.4f}, f1: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a2fbe4-37f9-4115-9775-65915efb3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained( \"C:\\\\Users\\\\Admin\\\\forex-sentiment-analysis\\\\models\\\\forex-analysis-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
